---
title: "分布式事务"
date: 2023-09-22T16:54:10+08:00
author: ["loveyu"]
draft: false
categories: 
- go
tags: 
- 分布式事务
- TTC
- SAGA
- 事务消息
---



来源文章:

[万字长文漫谈分布式事务实现原理](https://mp.weixin.qq.com/s/Z-ZY9VYUzNER8iwk80XSxA)

[从零到一搭建 TCC 分布式事务框架](https://mp.weixin.qq.com/s/aTQ6mgVUbUqn69NLmXh_7A)

[微软SAGA解答](https://learn.microsoft.com/zh-cn/azure/architecture/reference-architectures/saga/saga)

[10分钟说透Saga分布式事务](https://cloud.tencent.com/developer/article/1839642)

现有实现:

[DTM](https://dtm.pub/guide/start.html)

# 事务的核心特征ACID

1. ### Atomicity原子性

   1. 全部成功或者全部失败

2. ### Consistency一致性

   1. 宏观角度下的全局数据保持一致

3. ### Isolation隔离性

   1. 并发事务不互相干扰

4. ### Durability持久性

   1. 事务提交后，变更永久生效



# 什么是分布式事务

>``对两个或以上独立服务进行调用操作,保证这些操作同时成功或失败,在失败时要做到回滚操作,恢复到失败前的状态`` 
>
>例如订单创建操作,分别有账户系统和库存系统
>
>在处理一笔来自用户创建订单的请求时，需要执行两步操作：
>
>- 从账户系统中，扣减用户的账户余额
>- 从库存系统中，扣减商品的剩余库存
>
>从技术流程上来讲，两个步骤是相对独立的两个操作，底层涉及到的存储介质也是相互独立的，因此无法基于本地事务的实现方式,所以需要分布式事务解决问题



# 事务消息实现方案

## 概念

偏狭义的分布式事务解决方案是``基于消息队列`` MessageQueue（后续简称 MQ）实现的事务消息 Transaction Message.

``at least once : `` 投递到 MQ 中的消息能至少被下游消费者 consumer 消费到一次.

基于此，MQ 组件能够保证消息不会在消费环节丢失，但是无法解决消息的重复性问题. 因此，倘若我们需要追求精确消费一次的目标，则下游的 consumer 还需要``基于消息的唯一键执行幂等去重操作``，在 at least once 的基础上过滤掉重复消息，最终达到 ``exactly once`` 的语义.

依赖于 MQ 中 at least once 的性质，我们简单认为，只要把一条消息成功投递到 MQ 组件中，它就``一定被下游 consumer 端消费端``，至少不会发生消息丢失的问题.



## 场景

假如执行一个分布式事务，事务流程中包含需要在服务 A 中执行的动作 I 以及需要在服务 B 中执行的动作 II



## 方案

使用消息队列



## 流程

- 以服务 A 作为 MQ 生产方 producer，服务 B 作为 MQ 消费方 consumer
- 服务 A 首先在执行动作 I，执行成功后往 MQ 中投递消息，驱动服务 B 执行动作 II
- 服务 B 消费到消息后，完成动作 II 的执行



## 优点:

- 服务 A 和服务 B 通过 MQ 组件实现``异步解耦``，从而提高系统处理整个事务流程的吞吐量
- 当服务 A 执行 动作 I 失败后，可以选择不投递消息从而``熔断流程``，因此保证不会出现动作 II 执行成功，而动作 I 执行失败的不一致问题
- 基于 MQ at least once 的语义，服务 A 只要``成功消息的投递``，就可以相信服务 B ``一定能消费到该消息``，至少服务 B 能感知到动作 II 需要执行的这一项情报
- 依赖于 MQ 消费侧的 ``ack 机制``，可以实现服务 B 有限轮次的``重试能力``. 即当服务 B 执行动作 II 失败后，可以给予 MQ bad ack，从而通过消息重发的机制实现动作 II 的重试，提高动作 II 的执行成功率



## 缺点:

- 问题 1：服务 B 消费到消息执行动作 II 可能发生失败，即便依赖于 MQ 重试也无法保证动作一定能执行成功，此时缺乏令服务 A 回滚动作 I 的机制. 因此很可能出现动作 I 执行成功，而动作 II 执行失败的不一致问题
- 问题 2：在这个流程中，服务 A 需要执行的操作有两步：（1）执行动作 I；（2）投递消息. 这两个步骤本质上也无法保证原子性，即可能出现服务 A 执行动作 I 成功，而投递消息失败的问题.



## 优化

> ``对于问题1使用事务消息方案无法解决`` 
>
> 一下的方案均是为解决问题2提出



### 方案一

> ``先执行本地事务，后执行消息投递``

优势：**不会出现消息投递成功而本地事务执行失败的情况**. 这是因为在本地事务执行失败时，可以主动熔断消息投递的动作.

劣势：**可能出现本地事务执行成功而消息投递失败的问题.** 比如本地事务成功后，想要尝试执行消息投递操作时一直出现失败，最终消息无法发出. 此时由于本地事务已经提交，要执行回滚操作会存在着很高的成本.



### 方案二

>``先执行消息投递，后执行本地事务`` 

优势：**不会出现本地事务执行成功而消息投递失败的问题**. 因为在消息投递失败时，可以不开启本地事务的执行操作.

劣势：**可能出现消息投递成功而本地事务执行失败问题.** 比如消息投递成功后，本地事务始终无法成功执行，而消息一经发出，就已经覆水难收了.



### 方案三

>``基于本地事务包裹消息投递操作的实现方式`` 

流程: 

- 首先 begin transaction，开启本地事务
- 在事务中，执行本地状态数据的更新
- 完成数据更新后，不立即 commit transaction
- 执行消息投递操作
- 倘若消息投递成功，则 commit transaction
- 倘若消息投递失败，则 rollback transaction

产生的问题:

- 在和数据库交互的本地事务中，夹杂了和第三方组件的 IO 操作，可能存在``引发长事务的风险`` 
- 执行消息投递时，可能因为超时或其他意外原因，导致出现消息在事实上已投递成功，但 producer 获得的``投递响应发生异常的问题``，这样就会导致本地事务被误回滚的问题
- 在执行事务提交操作时，可能发生失败. 此时事务内的数据库修改操作自然能够回滚，然而 ``MQ 消息一经发出，就已经无法回收了``.



### *方案四

>``事务消息 Transaction Message.`` 
>
>以`` RocketMQ 中 TX Msg ``的实现方案为例展开介绍. 首先抛出结论，TX Msg 能保证我们做到在本地事务执行成功的情况下，后置的投递消息操作能以``接近百分之百的概率被发出``.
>
>``也就是对发送消息加上需要确认发送或者回滚消息的操作`` 

流程:

- producer 执行本地事务
- 如果本地事务执行成功，producer 直接提交本地事务，并且向 RocketMQ 发出一条确认消息
- 如果本地事务执行失败，producer 向 RocketMQ 发出一条回滚指令
- 倘若 RocketMQ 接收到确认消息，则会执行消息的发送操作，供下游消费者 consumer 消费
- 倘若 RocketMQ 接收到回滚指令，则会删除对应的半事务消息，不会执行实际的消息发送操作
- 此外，在 RocketMQ 侧，针对半事务消息会有一个轮询任务，倘若半事务消息一直未收到来自 producer 侧的二次确认，则 RocketMQ 会持续主动询问 producer 侧本地事务的执行状态，从而引导半事务消息走向终态

优点:

- 倘若本地事务执行失败，则 producer 会向 RocketMQ 发出删除半事务消息的回滚指令，因此保证消息不会被发出
- 倘若本地事务执行成功， 则 producer 会向 RocketMQ 发出事务成功的确认指令，因此消息能够被正常发出
- 倘若 producer 端在发出第二轮的确认或回滚指令前发生意外状况，导致第二轮结果指令确实. 则 RocketMQ 会基于自身的轮询机制主动询问本地事务的执行状况，最终帮助半事务消息推进进度.



缺点:

- **流程高度抽象：** TX Msg 把流程抽象成本地事务+投递消息两个步骤. 然而在实际业务场景中，分布式事务内包含的步骤数量可能很多，因此就需要把更多的内容更重的内容糅合在所谓的“本地事务”环节中，上游 producer 侧可能会存在比较大的压力
- **不具备逆向回滚能力：** 倘若接收消息的下游 consumer 侧执行操作失败，此时至多只能依赖于 MQ 的重发机制通过重试动作的方式提高执行成功率，但是无法从根本上解决下游 consumer 操作失败后回滚上游 producer 的问题. 这一点正是 TX Msg 中存在的最大的局限性.



# TCC

`（Two-Phase Commit with Timeout and Heartbeat）`

## 概念

>``TCC，全称 Try-Confirm-Cancel，指的是将一笔状态数据的修改操作拆分成两个阶段`` 

流程:

- 第一个阶段是 Try，指的是先对资源进行锁定，资源处于中间态但不处于最终态
- 第二个阶段分为 Confirm 和 Cancel，指的是在 Try 操作的基础上，真正提交这次修改操作还是回滚这次变更操作



![](https://www.loveyu.asia//img/640.png)



在 TCC 分布式事务架构中，包含三类角色：

- 应用方 Application：指的是需要使用到分布式事务能力的应用方，即这套 TCC 框架服务的甲方
- TCC 组件 TCC Component：指的是需要完成分布式事务中某个特定步骤的子模块. 这个模块通常负责一些状态数据的维护和更新操作，需要对外暴露出 Try、Confirm 和 Cancel 三个 API：
  - Try：锁定资源，通常以类似【冻结】的语义对资源的状态进行描述，保留后续变化的可能性
  - Confirm：对 Try 操作进行二次确认，将记录中的【冻结】态改为【成功】态
  - Cancel：对 Try 操作进行回滚，将记录中的【冻结】状消除或者改为【失败】态. 其底层对应的状态数据会进行回滚
- 事务协调器 TX Manager：负责统筹分布式事务的执行：
  - 实现 TCC Component 的注册管理功能
  - 负责和 Application 交互，提供分布式事务的创建入口，给予 Application 事务执行结果的响应
  - 串联 Try -> Confirm/Cancel 的两阶段流程. 在第一阶段中批量调用 TCC Component 的 Try 接口，根据其结果，决定第二阶段是批量调用 TCC Component 的 Confirm 接口还是 Cancel 接口



## 场景

每当有一笔支付请求到达，我们需要执行下述三步操作，并要求其前后状态保持一致性：

- 在订单模块中，创建出这笔订单流水记录
- 在账户模块中，对用户的账户进行相应金额的扣减
- 在库存模块中，对商品的库存数量进行扣减

>这三步操作分别需要对接订单、账户、库存三个不同的子模块，``底层的状态数据是基于不同的数据库和存储组件实现的``，并且我们这套后台系统是基于当前流行的微服务架构实现的，这三子个模块本身对应的就是三个相互独立的微服务，因此如何实现在一笔支付请求处理流程中，使得这三笔操作对应的状态数据始终保持高度一致性，就成了一个非常具有技术挑战性的问题.



## 方案

首先，我们基于 TCC 的设计理念，将订单模块、账户模块、库存模块``分别改造成三个 TCC Component``，每个 Component 对应需要暴露出`` Try、Confirm、Cancel 三个 API``，对应于冻结资源、确认更新资源、回滚解冻资源三个行为.

同时，为了能够简化后续 TX Manager 和 Application 之间的交互协议，``每个 TCC Component 会以插件的形式提前注册到 TX Manager 维护的组件市场 Component Market 中``，并提前声明好一个全局唯一键与之进行映射关联.

![](https://www.loveyu.asia//img/640-20230923155552557.png)

由于每个 TCC Component 需要支持 Try 接口的锁定操作，因此其中维护的数据需要在明细记录中拆出一个用于标识 “冻结” 状态的标签，或者在状态机中拆出一个 “冻结” 状态.

最终在第二阶段的 Confirm 或者 Cancel 请求到达时，再把 ”冻结“ 状态调整为 ”成功“ 或者 ”失败“ 的终态.

![](https://www.loveyu.asia//img/640-20230923155643457.png)



## 流程

- Application 调用 TX Manager 的接口，创建一轮分布式事务：
- Application 需要向 TX Manager 声明，这次操作涉及到的 TCC Component 范围，包括 订单组件、账户组件和库存组件
- Application 需要向 TX Manager 提前传递好，用于和每个 TCC Component 交互的请求参数（ TX Manager 调用 Component Try 接口时需要传递）
- TX Manager 需要为这笔新开启的分布式事务分配一个全局唯一的事务主键 Transaction ID
- TX Manager 将这笔分布式事务的明细记录添加到事务日志表中
- TX Manager 分别调用订单、账户、库存组件的 Try 接口，试探各个子模块的响应状况，比并尝试锁定对应的资源
- TX Manager 收集每个 TCC Component Try 接口的响应结果，根据结果决定下一轮的动作是 Confirm 还是 Cancel
- 倘若三笔 Try 请求中，有任意一笔未请求成功：
- TX Manager 给予 Application 事务执行失败的 Response
- TX Manager 批量调用订单、账户、库存 Component 的 Cancel 接口，回滚释放对应的资源
- 在三笔 Cancel 请求都响应成功后，TX Manager 在事务日志表中将这笔事务记录置为【失败】状态
- 倘若三笔 Try 请求均响应成功了：
- TX Manager 给予 Application 事务执行成功的 ACK
- TX Manager 批量调用订单、账户、库存 Component 的 Confirm 接口，使得对应的变更记录实际生效
- 在三笔 Confirm 请求都响应成功后，TX Manager 将这笔事务日志置为【成功】状态 

![](https://www.loveyu.asia//img/640-20230923155858297.png)

## 说明

首先，TCC 本质上是一个两阶段提交（Two Phase Commitment Protocol，2PC）的实现方案，分为 Try 和 Confirm/Cancel 的两个阶段：

- ``Try 操作的容错率是比较高的``，原因在于有人帮它兜底. Try 只是一个试探性的操作，不论成功或失败，后续可以通过第二轮的 Confirm 或 Cancel 操作对最终结果进行修正
- ``Confirm/Cancel 操作是没有容错的``，倘若在第二阶段出现问题，可能会导致 Component 中的状态数据被长时间”冻结“或者数据状态不一致的问题

针对于这个场景，TCC 架构中采用的解决方案是：``在第二阶段中，TX Manager 轮询重试 + TCC Component 幂等去重. 通过这两套动作形成的组合拳，保证 Confirm/ Cancel 操作至少会被 TCC Component 执行一次.``

首先，针对于 TX Manager 而言：

- 需要启动一个定时轮询任务
- 对于事务日志表中，所有未被更新为【成功/失败】对应终态的事务，需要摘出进行检查
- 检查时查看其涉及的每个组件的 Try 接口的响应状态以及这笔事务的持续时长
- 倘若事务应该被置为【失败】（存在某个 TCC Component Try 接口请求失败），但状态却并未更新，说明之前批量执行 Cancel 操作时可能发生了错误. 此时需要补偿性地批量调用事务所涉及的所有 Component 的 Cancel 操作，待所有 Cancel 操作都成功后，将事务置为【失败】状态
- 倘若事务应该被置为【成功】（所有 TCC Component Try 接口均请求成功），但状态却并未更新，说明之前批量执行 Confirm 操作时可能发生了错误. 此时需要补偿性地批量调用事务所涉及的所有 Component 的 Confirm 操作，待所有 Confirm 操作都成功后，将事务置为【成功】状态
- 倘若事务仍处于【进行中】状态（TCC Component Try 接口请求未出现失败，但并非所有 Component Try 接口都请求成功），则检查事务的创建时间，倘若其耗时过长，同样需要按照事务失败的方式进行处理

需要注意，在 TX Manager 轮询重试的流程中，针对下游 TCC Component 的 Confirm 和 Cancel 请求只能保证 at least once 的语义，换句话说，这部分请求是可能出现重复的.

因此，``在下游 TCC Component 中，需要在接收到 Confirm/Cancel 请求时，执行幂等去重操作``. 幂等去重操作需要有一个唯一键作为去重的标识，这个标识键就是 TX Manager 在``开启事务时为其分配的全局唯一的 Transaction ID``，它既要作为这项事务在事务日志表中的唯一键，同时在 TX Manager 每次向 TCC Component 发起请求时，都要``携带上这笔 Transaction ID.``

![](https://www.loveyu.asia//img/640-20230923160311120.png)



## TX Manager 职责

- ``暴露出注册 TCC Component 的接口，进行 Component 的注册和管理``
- 暴露出启动分布式事务的接口，``作为和 Application 交互的唯一入口``，并基于 Application 事务执行结果的反馈
- 为每个事务``维护全局唯一的 Transaction ID``，基于事务日志表记录每项分布式事务的进展明细
- ``串联 Try——Confirm/Cancel 的两阶段流程``，根据 Try 的结果，推进执行 Confirm 或 Cancel 流程
- ``持续运行轮询检查任务，推进每个处于中间态的分布式事务流转到终态``



## TCC Component 职责

![](https://www.loveyu.asia//img/640-20230923160527271.png)

- ``暴露出 Try、Confirm、Cancel 三个入口，对应于 TCC 的语义``
- 针对数据记录，新增出一个对应于 Try 操作的中间状态枚举值
- 针对于同一笔事务的重复请求，需要执行幂等性校验
- 需要支持空回滚操作. 即针对于一笔新的 Transaction ID，在没收到 Try 的前提下，若提前收到了 Cancel 操作，也需要将这个信息记录下来，但不需要对真实的状态数据发生变更



## 空悬挂

>``本应先执行try后confirm/cancel,但因为网络问题TX Manager执行try后长时间没得到回应当做失败执行cancel但是执行cancel后try操作到达Component即为空悬挂`` 

- TX Manager 在向 Component A 发起 Try 请求时，由于出现网络拥堵，导致请求超时
- TX Manager 发现存在 Try 请求超时，将其判定为失败，因此批量执行 Component 的 Cancel 操作
- Component A 率先收到了后发先至的 Cancel 请求
- 过了一会儿，之前阻塞在网络链路中的 Try 请求也到达了 Component A

从执行逻辑上，Try 应该先于 Cancel 到达和处理，然而在事实上，由于网络环境的不稳定性，请求到达的先后次序可能颠倒. 在这个场景中，Component A 需要保证的是，针对于同一笔事务，只要接受过对应的 Cancel 请求，之后到来的 Try 请求需要被忽略. 这就是 TCC Component 需要支持空回滚操作的原因所在.

![](https://www.loveyu.asia//img/640-20230923160807495.png)



## 优势

- TCC 可以称得上是真正意义上的分布式事务：任意一个 Component 的 Try 操作发生问题，都能支持事务的整体回滚操作
- TCC 流程中，分布式事务中数据的状态一致性能够趋近于 100%，这是因为第二阶段 Confirm/Cancel 的成功率是很高的，原因在于如下三个方面：
- TX Manager 在此前刚和 Component 经历过一轮 Try 请求的交互并获得了成功的 ACK，因此短时间内，Component 出现网络问题或者自身节点状态问题的概率是比较小的
- TX Manager 已经通过 Try 操作，让 Component 提前锁定了对应的资源，因此确保了资源是充分的，且由于执行了状态锁定，出现并发问题的概率也会比较小
- TX Manager 中通过轮询重试机制，保证了在 Confirm 和 Cancel 操作执行失败时，也能够通过重试机制得到补偿



## 劣势

- TCC 分布式事务中，涉及的状态数据变更只能趋近于最终一致性，无法做到即时一致性
- 事务的原子性只能做到趋近于 100%，而无法做到真正意义上的 100%，原因就在于第二阶段的 Confirm 和 Cancel 仍然存在极小概率发生失败，即便通过重试机制也无法挽救. 这部分小概率事件，就需要通过人为介入进行兜底处理
- TCC 架构的实现成本是很高的，需要所有子模块改造成 TCC 组件的格式，且整个事务的处理流程是相对繁重且复杂的. 因此在针对数据一致性要求不那么高的场景中，通常不会使用到这套架构.



# SAGA

`（Saga Pattern）`

## 概念

``每个事务存在多个子事务，每个子事务都有一个补偿事务，其在事务回滚的时候使用。``

相对于TTC的优点:``更适合业务流程长、业务流程多,无法实现try,confirm,cancel接口的事务操作``  

优点: 一阶段提交本地事务，无锁，高性能；参与者可异步执行，高吞吐；补偿服务易于实现，因为一个更新操作的反向操作是比较容易理解的

缺点: 就是不保证隔离性。

``每个Saga由一系列sub-transaction Ti组成。``每个Ti都有对应的补偿动作Ci，补偿动作用于撤销Ti造成的结果。这里可以理解为，针对每一个分布式事务的每个执行操作或者是步骤都是一个 Ti，例如扣减库存是T1、创建订单是T2、支付服务是T3。那么针对每个Ti都对应一个补偿动作Ci，例如回复库存C1、订单回滚C2、支付回滚C3。



## 恢复策略

>``对于执行不通过的事务，会尝试重试事务，这里有一个假设就是每个子事务最终都会成功。这种方式适用于必须要成功的场景`` 

例如执行操作为: T1 -> T2 -> T3 如果执行到T3失败则恢复顺序为

### 向前恢复（forward recovery

C1 -> C2 -> C3



### 向后恢复（backward recovery）

C3 -> C2 -> C1



## 事务协调

### 协调

> 协调是协调 Saga 的一种方法，参与者在没有集中控制点的情况下交换事件。 通过协调，每个本地事务都会发布在其他服务中触发本地事务的域事件。

![](https://www.loveyu.asia//img/choreography-pattern.png)



####  好处

- 适用于只需很少参与者且不需要协调逻辑的简单工作流。
- 不需要额外的服务实现和维护。
- 不会引入单一故障点，因为责任在各个 Saga 参与者之间进行分配。



#### 缺点

- 添加新步骤时，工作流可能会变得混乱，因为很难跟踪哪些 Saga 参与者侦听哪些命令。
- 由于 Saga 参与者必须使用彼此的命令，因此他们之间存在循环依赖的风险。
- 集成测试很困难，因为必须运行所有服务才能模拟事务。



### 编排

>编排是协调 Saga 的一种方法，在此方法中，中央控制器告诉 Saga 参与者要执行的本地事务。 Saga 编排器处理所有事务，并告知参与者根据事件执行哪项操作。 编排器执行 Saga 请求、存储和解释每个任务的状态，并通过补偿事务处理故障恢复。

![](https://www.loveyu.asia//img/orchestrator.png)

#### 好处

- 适用于涉及许多参与者或随时间推移增加了新参与者的复杂工作流。
- 适用于可控制流程中的每个参与者和活动流的情况。
- 不会引入循环依赖项，因为编排器单方面依赖于 Saga 参与者。
- Saga 参与者无需了解其他参与者的命令。 明确的关注点分离可简化业务逻辑。



#### 缺点

- 其他设计复杂性需要协调逻辑的实现。
- 还有一个额外的故障点，因为编排器管理完整的工作流。



# XA

`（Two-Phase Commit）`

XA协议是一种两阶段提交协议。在XA中，有一个主事务管理器和多个参与者。事务管理器协调并控制多个参与者的操作。XA协议通过预提交和确认提交两个阶段来保证事务的一致性。

- 预提交阶段：参与者将待提交的操作记录在事务日志中，等待主事务管理器的指令。
- 提交阶段：主事务管理器收到所有参与者的预提交请求后，发送提交指令给各个参与者。如果所有参与者都能成功提交，则主事务管理器发送确认提交指令给各个参与者，完成整个事务的提交。如果任何参与者在提交阶段遇到问题，主事务管理器发送回滚指令给各个参与者，放弃整个事务的提交。



# AT

`（Application-Level Transaction）` 

AT协议是一种应用层面的事务处理方式，相对于XA协议更加灵活。AT协议通过在应用程序中嵌入事务逻辑，将事务操作划分为多个小步骤，并在每个步骤之后记录操作的状态，以便在失败时进行回滚。AT协议通常使用补偿机制来实现事务的回滚，在发生失败时执行相反操作进行补偿，恢复到事务开始之前的状态。



# 总结

## Transaction Message

能够支持狭义的分布式事务. 基于消息队列组件中半事务消息以及轮询检查机制，保证了本地事务和消息生产两个动作的原子性，但不具备事务的逆向回滚能力

## TCC Transaction

能够支持广义的分布式事务. 架构中每个模块需要改造成实现 Try/Confirm/Cancel 能力的 TCC 组件，通过事务协调器进行全局 Try——Confirm/Cancel 两阶段流程的串联，保证数据的最终一致性趋近于 100%



## SAGA

在以下情况下，请使用 Saga 模式：

- 需确保分布式系统中的数据一致性，而无需紧密耦合。
- 需要在序列中的某个操作失败时进行回滚或补偿。

Saga 模式不太适合用于：

- 紧密耦合事务。
- 补偿早期参与者中发生的事务。
- 循环依赖项。



